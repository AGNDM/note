凸函数 (Convex Function)
定义
函数 $f$ 是凸函数当且仅当：
1.$dom(f)$ 是一个凸集。
2.对任意 $x_1, x_2 \in dom(f)$ 且 $\theta \in$，有：$f(\theta x_1 + (1−\theta)x_2) \leq \theta f(x_1) + (1−\theta)f(x_2)$。
凸函数： 仿射函数 ($ax + b$)、指数函数 ($e^{ax}$)、幂函数 ($x^p$，当 $p \geq 1$ 或 $p \leq 0$)、绝对值幂函数 ($|x|^p$，当 $p \geq 1$)、负熵函数 ($x \ln x$)。
仿射函数既是凸的也是凹的。
范数是凸函数，如一般的 $l_p$ 范数和谱范数。
验证函数的凸性方法
一阶条件 (First-Order Condition)：可微函数 $f$ 是凸的，当且仅当它的定义域是凸的，且满足 $f(x) + \nabla f(x)^\top (y - x) \leq f(y)$ 对所有 $x, y \in dom(f)$。
二阶条件 (Second-Order Condition)：对于定义域为凸集的二阶可微函数 $f$，有 $f$ 是凸函数当且仅当 $\nabla^2 f(x) \succeq 0$ 对所有 $x \in dom(f)$。若 $\nabla^2 f(x) \succ 0$，则 $f$ 是严格凸函数。
例子：二次函数 $f(x) = \frac{1}{2}x^\top Px + q^\top x + r$ 当 $P \succeq 0$ 时是凸的。最小二乘目标函数 $f(x) = |Ax - b|_2$ 对任意 $A$ 都是凸的。
归约为标量函数 (Reduction to a Scalar Function)：函数 $f: \mathbb{R}^n \to \mathbb{R}$ 是凸函数，当且仅当 $dom(f)$ 是凸集，且函数 $g(t) = f(x + tv)$ 对任意 $x \in dom(f), v \in \mathbb{R}^n$ 是关于 $t$ 的凸函数。
上图函数法 (Epigraph)：函数 $f$ 是凸的，当且仅当其上图集 $epi f = {(x, t) \in \mathbb{R}^{n+1} \mid x \in dom(f), f(x) \leq t}$ 是凸集。
保持凸性的运算 (Operations Preserving Convexity)
正数缩放 (Positive Scaling)：若 $f$ 是凸函数，$\lambda \geq 0$，则 $\lambda f$ 是凸函数。
求和 (Sum)：对于凸函数 $f_1$ 和 $f_2$，它们的和 $f_1 + f_2$ 是凸的。
与仿射函数的复合 (Composition with Affine Function)：若 $f$ 是凸函数，则 $f(Ax + b)$ 是凸函数。
逐点最大值 (Pointwise Maximum)：若 $f_1, \dots, f_m$ 是凸函数，则 $f(x) = \max{f_1(x), \dots, f_m(x)}$ 是凸函数。
逐点上确界 (Pointwise Supremum)：若 $f(x, y)$ 对于每个 $y \in A$ 是关于 $x$ 的凸函数，则 $g(x) = \sup_{y \in A} f(x, y)$ 是凸函数。
与标量函数的复合 (Composition with Scalar Functions)：设 $g: \mathbb{R}^n \to \mathbb{R}$, $h: \mathbb{R} \to \mathbb{R}$，定义复合函数 $f(x) = h(g(x))$。则 $f$ 是凸函数，当满足以下条件之一：
$g$ 是凸函数，$h$ 是非减且凸函数。
$g$ 是凹函数，$h$ 是非增且凸函数。
与向量函数的复合 (Composition with Vector Functions)：设 $g: \mathbb{R}^n \to \mathbb{R}^k$, $h: \mathbb{R}^k \to \mathbb{R}$，定义复合函数 $f(x) = h(g_1(x), \dots, g_k(x))$。则 $f$ 是凸函数，当满足以下条件之一：
$g_i$ 是凸函数，$h$ 在每个变量上都是凸且非减的。
$g_i$ 是凹函数，$h$ 在每个变量上都是凸且非增的。
最小化 (Minimization)：若 $f(x, y)$ 关于 $(x, y)$ 是凸函数，且 $C$ 是一个凸集合，则 $g(x) = \inf_{y \in C} f(x, y)$ 是关于 $x$ 的凸函数。
拟凸性 (Quasi-Convexity)
函数 $f: \mathbb{R}^n \to \mathbb{R}$ 是拟凸函数，当且仅当 $dom(f)$ 是凸集，且其下水平集 $S_\alpha = {x \in dom(f) \mid f(x) \leq \alpha}$ 对所有 $\alpha$ 都是凸集。
另一种定义：当 $dom(f)$ 是凸的，且对于任意 $x, y \in dom(f)$ 与 $\theta \in$，有 $f(\theta x + (1−\theta)y) \leq \max{f(x), f(y)}$。
共轭函数 (The Conjugate Function)
函数 $f$ 的共轭函数 $f^\ast(y) = \sup_{x \in dom(f)} (y^\top x - f(x))$。
$f^\ast$ 总是凸函数（即使 $f$ 不是凸的）。
Fenchel 不等式 (Fenchel's Inequality)：$f(x) + f^\ast(y) \geq y^\top x$, $\forall x, y$。
如果 $f$ 是凸函数且其上图集是闭集，则 $f^{\ast\ast} = f$。
凸集合 (Convex Set)
定义
一个集合中任意两点之间的线段也包含在该集合中：$\forall x_1, x_2 \in C, 0 \leq \theta \leq 1 \Rightarrow \theta x_1 + (1-\theta)x_2 \in C$。
超平面与半空间 (Hyperplanes and Halfspaces)
超平面 (Hyperplane)：${x \in \mathbb{R}^n : a^\top x = b}$ ($a \ne 0$)。
半空间 (Halfspace)：${x \in \mathbb{R}^n : a^\top x \leq b}$ ($a \ne 0$)。
多面体 (Polyhedra)：${x \in \mathbb{R}^n : Ax \leq b}$。
欧几里得球与椭球体 (Euclidean Balls and Ellipsoids)
范数球与范数锥 (Norm Balls and Norm Cones)
范数锥 (Norm Cone)：${(x, t) : |x| \leq t}$。
欧几里得范数锥也被称为二阶锥 (Second-Order Cone)。
凸组合与凸包 (Convex Combination and Convex Hull)
凸组合 (Convex combination)：$x = \theta_1 x_1 + \dots + \theta_k x_k$ 满足 $\theta_1 + \dots + \theta_k = 1, \theta_i \geq 0$。
凸包 (Convex hull) $conv(S)$：集合 $S$ 中所有点的凸组合构成的集合。
线性子空间一定是凸锥；凸锥一定是凸集。
保持凸性的运算 (Operations Preserving Convexity)
交集 (Intersection)：$C_1 \cap C_2$。
和 (Sum)：$C_1 + C_2$ (Minkowski 和)。
平移 (Translated set)：$C + a$。
缩放 (Scaled set)：$tC$。
笛卡尔积 (Cartesian product)：$C_1 \times C_2$。
分离超平面定理 (Separating Hyperplane Theorem)
如果 $C$ 和 $D$ 是非空不相交的凸集，则存在 $a \ne 0, b$ 使得 $a^\top x \leq b$ 对所有 $x \in C$ 且 $a^\top x \geq b$ 对所有 $x \in D$。
支撑超平面定理 (Supporting Hyperplane Theorem)
如果 $C$ 是凸集，则在 $C$ 的每一个边界点都存在一个支撑超平面。
凸优化问题 (Convex Optimization Problem)
优化问题的标准形式
$\min f_0(x)$ s.t. $f_i(x) \leq 0, i = 1, \dots, m$; $h_i(x) = 0, i = 1, \dots, p$。
可行解 (Feasible solution)：$x \in dom(f_0)$ 且满足约束。
最优解 (Optimal solution)：可行解 $x$ 满足 $f_0(x) = p^\star$ (最优值)。
局部最优解 (Local optimal solution)：在某个半径 $R > 0$ 范围内为最优解。
凸优化问题
定义：标准形式中，目标函数 $f_0$ 和不等式约束函数 $f_1, \dots, f_m$ 都是凸函数，等式约束是仿射的 ($Ax=b$)。
重要性质：凸优化问题的可行解集是凸集。
局部最优与全局最优：任意凸优化问题的局部最优点也是全局最优点。
可微函数的最优性判据
无约束问题 $\min f_0(x)$：最优的充要条件为 $\nabla f_0(x) = 0$。
等式约束问题 $\min f_0(x)$ s.t. $Ax=b$：最优性充要条件为存在 $\nu$ 使得 $x \in dom(f_0), Ax=b, \nabla f_0(x) + A^\top \nu = 0$。
非负正交域上的最优化 $\min f_0(x)$ s.t. $x \geq 0$：最优性充要条件为 $x \in dom(f_0), x \succeq 0$, 且 $\nabla f_0(x)_i \geq 0$ 若 $x_i = 0$，$\nabla f_0(x)_i = 0$ 若 $x_i > 0$。
等价凸优化问题：可以通过消除/引入等式约束、引入松弛变量、改写成上图形式或对部分变量先进行最小化来重构等价凸问题。
拟凸优化问题 (Quasiconvex Optimization)
定义：目标函数 $f_0$ 是拟凸函数，$f_1, \dots, f_m$ 是凸函数，$Ax=b$ 是仿射约束。
求解：通过凸可行性问题 + 二分法求解。
凸问题常见形式
线性规划 (Linear Program, LP)：目标函数和约束函数均为仿射函数。
二次规划 (Quadratic Program, QP)：目标函数是凸二次函数，约束为仿射函数。要求二次项矩阵 $P \in \mathbb{S}^n_+$。
带二次约束的二次规划 (QCQP)：目标函数和约束均为凸二次函数。要求二次项矩阵 $P_i \in \mathbb{S}^n_+$。
二阶锥规划 (Second-order Cone Programming, SOCP)：包含二阶锥约束（范数约束），比 LP 和 QCQP 更一般。可用于建模鲁棒线性规划。
半正定规划 (Semidefinite Program, SDP)：包含线性矩阵不等式约束。LP 和 SOCP 均可表达为 SDP。
特征值最小化问题 (Eigenvalue Minimization)：$\min \lambda_{max}(A(x))$ 可等价转化为 SDP。
对偶理论 (Duality Theory)
拉格朗日函数 (Lagrangian Function)
标准优化问题 $\min f_0(x)$ s.t. $f_i(x) \leq 0, h_i(x) = 0$ 的拉格朗日函数为 $L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x)$。
$\lambda_i, \nu_i$ 是拉格朗日乘子。
拉格朗日对偶函数 (Lagrange Dual Function)
$g(\lambda, \nu) = \inf_{x \in D} L(x, \lambda, \nu)$。
函数 $g$ 是凹函数。
下界性质 (Lower bound property)：若 $\lambda \geq 0$，则 $g(\lambda, \nu) \leq p^\star$ (原始问题最优值)。
对偶问题 (Dual Problem)
$\max g(\lambda, \nu)$ s.t. $\lambda \geq 0$。
对偶问题是凸问题，最优值用 $d^\star$ 表示，我们实际上是在最大化最小值。
弱对偶 (Weak Duality)
$d^\star \leq p^\star$。
总是成立（无论问题是凸的还是非凸的），可用于为困难问题提供下界。
强对偶 (Strong Duality)
$d^\star = p^\star$。
一般不成立，但对于凸优化问题通常成立。
保证强对偶成立的条件被称为约束资格条件 (Constraint Qualifications)。
Slater 约束资格条件 (Slater's Constraint Qualification)
对于凸优化问题，如果存在一个严格可行的点 $x \in int D$ 使得 $f_i(x) < 0, \forall i$ 且 $Ax = b$，那么该问题强对偶成立。
若 $p^\star > -\infty$，该条件也保证对偶最优解存在。
互补松弛性 (Complementary Slackness)
若强对偶成立，$x^\star$ 为原始问题最优解，$(\lambda^\star, \nu^\star)$ 为对偶问题最优解，则对所有 $i = 1, \dots, m$，有 $\lambda^\star_i f_i(x^\star) = 0$。
这意味着：若 $\lambda^\star_i > 0 \Rightarrow f_i(x^\star) = 0$ (约束被激活)，若 $f_i(x^\star) < 0 \Rightarrow \lambda^\star_i = 0$ (约束未激活)。
Karush-Kuhn-Tucker (KKT) 条件
对于可微优化问题，KKT 条件是以下四个条件的集合：
1.原始约束 (Primal constraints)：$f_i(x) \leq 0, h_i(x) = 0$。
2.对偶可行性 (Dual constraints)：$\lambda \geq 0$。
3.互补松弛性 (Complementary slackness)：$\lambda_i f_i(x) = 0$。
4.拉格朗日函数关于 $x$ 的梯度为零 (Stationarity)：$\nabla f_0(x) + \sum_{i=1}^m \lambda_i \nabla f_i(x) + \sum_{i=1}^p \nu_i \nabla h_i(x) = 0$。
若强对偶成立且 $x, \lambda, \nu$ 为最优解，则它们必须满足 KKT 条件。
对于凸优化问题，若 $x, \lambda, \nu$ 满足 KKT 条件，则它们是最优解。
若满足 Slater 条件，则 $x$ 是最优的当且仅当存在 $\lambda, \nu$ 满足 KKT 条件。
扰动与敏感性分析 (Perturbation and Sensitivity Analysis)
若未扰动问题满足强对偶性，且 $\lambda^\star, \nu^\star$ 是其对偶最优解，则扰动问题的最优值 $p^\star(u, v) \geq p^\star(0, 0) - u^\top \lambda^\star - v^\top \nu^\star$。
局部敏感性 (Local Sensitivity)：如果 $p^\star(u, v)$ 在 $(0, 0)$ 处可微，则 $\lambda^\star_i = -\frac{\partial p^\star(0, 0)}{\partial u_i}$, $\nu^\star_i = -\frac{\partial p^\star(0, 0)}{\partial v_i}$。拉格朗日乘子表示约束水平变化对最优值的影响率。
迭代算法 (Iterative Algorithms)
迭代方法：通过初始值生成一系列逐步改进的近似解的数学过程。
三个关键要素：初始点 $x_0$、更新策略 $\Phi(\cdot)$、终止准则。
终止准则
梯度足够小：$|\nabla f(x_k)| \leq \epsilon$。
目标函数变化量足够小：$|f(x_{k+1}) - f(x_k)| \leq \epsilon$ 或相对变化量足够小。
迭代点变化量足够小：$|x_{k+1} - x_k| \leq \epsilon$ 或相对变化量足够小。
收敛速率 (Convergence Rates)
Q-收敛定义：$\lim_{k \to \infty} \frac{e_{k+1}}{e_k} = \mu$。
次线性收敛：$\mu = 1$。误差按 $O(1/k)$ 或 $O(1/k^2)$ 衰减。
线性收敛：$\mu \in (0, 1)$。误差按 $O(\mu^k)$ 衰减，达到精度 $\epsilon$ 需要 $O(\log(1/\epsilon))$ 次迭代。
超线性收敛：$\mu = 0$。
二次收敛：$\lim_{k \to \infty} \frac{e_{k+1}}{(e_k)^2} = \mu > 0$。在实践中通常只需常数次迭代。
R-收敛：若对任意 $k$ 有 $|e_k| \leq \epsilon_k$ 且 ${\epsilon_k}$ Q-收敛于 0，则称 ${x_k}$ 为 R-收敛。
一阶方法 (First-Order Methods)
梯度下降法 (Gradient Descent)
算法：$x_{k+1} = x_k - t_k \nabla f(x_k)$。
$-\nabla f(x_k)$ 称为最速下降方向。
步长选择规则：固定步长、精确线搜索、回溯线搜索 (Armijo 准则)。
![[Pasted image 20250622215554.png]]
回溯线搜索
![[Pasted image 20250622231825.png]]
L-光滑函数 (L-Smooth Function)
定义：$|\nabla f(x) - \nabla f(y)|_2 \leq L |x - y|_2$。等价于 $f(y) \leq f(x) + \nabla f(x)^\top (y - x) + \frac{L}{2} |y - x|_2^2$。
收敛性 (非凸)：找到 $\epsilon$-稳定点 $(|\nabla f(x)| \leq \epsilon)$ 需要 $O(1/\epsilon^2)$ 次迭代。
凸且 L-光滑函数
收敛性：找到 $\epsilon$-次优解 $(|f(x_k) - f^\star| \leq \epsilon)$ 需要 $O(1/\epsilon)$ 次迭代。
$\mu$-强凸且 L-光滑函数
定义：$f(x) - \frac{\mu}{2} x^\top x$ 是凸函数。等价于 $f(y) \geq f(x) + \nabla f(x)^\top (y - x) + \frac{\mu}{2} |x - y|_2^2$。
收敛性：线性收敛，达到 $\epsilon$-次优点需要 $O(\log(1/\epsilon))$ 次迭代。
条件数 (Condition Number) $L/\mu$：影响梯度下降速度的主要因素。当 $L/\mu$ 较大时，问题称为病态条件问题。
次梯度下降法 (Subgradient Descent)
次梯度 (Subgradient) $g$：对于凸函数 $f$，若 $f(y) \geq f(x) + g^\top (y - x)$ 对所有 $y$ 成立，则称 $g$ 是 $f$ 在 $x$ 处的次梯度。
次微分 (Subdifferential) $\partial f(x)$：所有次梯度的集合。
无约束问题最优性条件：$x^\star$ 是最优解当且仅当 $0 \in \partial f(x^\star)$。
算法：$x_{k+1} = x_k - t_k g_k$，其中 $g_k \in \partial f(x_k)$。
步长选择：固定步长或递减步长。
收敛性：
固定步长：$f_{best}^k - f^\star \leq \frac{R^2}{2kt} + \frac{G^2t}{2}$，取$t=\frac{R}{g\sqrt{ K }}$收敛到 $O(1/\sqrt{K})$ 但不保证收敛到最优解。
递减步长：若 $t_k \to 0, \sum t_k = \infty$，则保证收敛到最优解。
Polyak 步长 ($t_k = \frac{f(x_k) - f^\star}{|g_k|_2^2}$，需要知道 $f^\star$)：收敛到 $O(GR/\sqrt{k})$。
投影梯度法 (Projected Gradient Method)
用于求解 $\min_{x \in C} g(x)$ (g 可微，C 凸集)。
算法：$x_{k+1} = P_C(x_k - t_k \nabla g(x_k))$。
投影算子 (Projection Operator) $P_C(x) = \arg \min_{u \in C} |x - u|_2$。
收敛性：与梯度下降法相似。
近端梯度法 (Proximal Gradient Method)
用于求解 $\min f(x) = g(x) + h(x)$ (g 可微，h 凸但不可微)。
算法：$x_{k+1} = \arg \min_u \left( g(x_k) + \nabla g(x_k)^\top (u - x_k) + \frac{1}{2t_k} |u - x_k|_2^2 + h(u) \right)$。
近端映射 (Proximal Mapping) $prox_h(x) = \arg \min_u \left( \frac{1}{2} |x - u|_2^2 + h(u) \right)$。
算法可表示为：$x_{k+1} = prox_{t_k h} (x_k - t_k \nabla g(x_k))$。
收敛性：
凸函数：O(1/k)。
强凸函数：线性收敛。
优点：适用于可微与不可微凸函数之和的优化问题；收敛性质与标准梯度法相似；虽不如次梯度法通用，但收敛速度更快。
![[Pasted image 20250622235224.png]]
FISTA (Fast Iterative Shrinkage-Thresholding Algorithm)
加速的近端梯度法，利用动量项。
收敛性：O(1/k^2)。
随机梯度下降法 (Stochastic Gradient Descent, SGD)
用于最小化求和形式的目标函数 $\min \frac{1}{n} \sum_{i=1}^n f_i(x)$ 或期望形式 $\min E_\omega f(x; \omega)$。
算法：$x_k = x_{k-1} - t_k \cdot \nabla f_{i_k}(x_{k-1})$ (每次迭代随机选择一个索引 $i_k$)。
小批量 (Mini-batch) SGD：$x_k = x_{k-1} - t_k \cdot \frac{1}{b} \sum_{i \in I_k} \nabla f_i(x_{k-1})$ (随机选择一个大小为 $b$ 的子集 $I_k$)。
随机梯度估计是无偏的：$E[\nabla f_{i_k}(x)] = \nabla f(x)$。
收敛性：
凸函数且 Lipschitz 梯度：期望收敛速率 $E[f(x_k)] - f^\star = O(1/\sqrt{k})$。
强凸函数：期望收敛速率 $E[f(x_k)] - f^\star = O(1/k)$ (不如全梯度下降的线性收敛)。
方差缩减方法 (Variance Reduction Methods)：旨在降低随机梯度的方差。
SAG (Stochastic Average Gradient)：维护梯度的平均值，使用有偏但方差缩减的估计量。
收敛性：凸函数下 O(1/k)，强凸函数下线性收敛 O($\rho^k$)。
SAGA：与 SAG 类似，但使用无偏方差缩减估计量。
收敛性：匹配 SAG 的强凸线性收敛速率。
自适应步长 (Adaptive Steplength)：如 AdaGrad、AdaDelta、RMSProp、Adam 等。
二阶方法 (Second-Order Methods)
牛顿法 (Newton's Method)
牛顿步 (Newton Step) $\Delta x_{nt} = -\nabla^2 f(x)^{-1} \nabla f(x)$。
牛顿步最小化目标函数的二阶泰勒近似。
算法：$x^{k+1}=x+\Delta x_{nt}$
局部收敛性：在接近最优解时表现优越，通常是超线性甚至二次收敛（如果 Hessian Lipschitz 连续）。
阻尼牛顿法 (Damped Newton Method)：使用线搜索来确定步长，以保证全局收敛。
牛顿递减量 (Newton Decrement) $\lambda(x) = (\nabla f(x)^\top \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}$：衡量 $x$ 与最优解接近程度的指标。
经典收敛性分析：包含阻尼牛顿阶段（线性收敛）和二次收敛阶段（二次收敛）。
仿射不变性 (Affine Invariance)：牛顿法对问题进行放缩（条件数）不敏感。
自和谐 (Self-concordance)：一种特殊类别的凸函数，牛顿法对其具有更强的收敛性保证。
信赖域方法 (Trust Region Methods)：另一种确定步长的方法，在局部二次模型上增加一个信任区域约束。
优点：收敛速度快，不受条件数影响。缺点：每次迭代计算量大 (需要 Hessian 矩阵的逆或求解线性系统，通常是 $O(n^3)$)。
拟牛顿法 (Quasi-Newton Methods)
动机：避免计算二阶导数和求解大型线性系统。
算法：$x_{k+1} = x_k - t H_k^{-1} \nabla f(x_k)$，其中 $H_k$ 是 Hessian 矩阵的近似。
割线条件 (Secant Condition)：$H_k (x_k - x_{k-1}) = \nabla f(x_k) - \nabla f(x_{k-1})$。
BFGS 更新 (Broyden-Fletcher-Goldfarb-Shanno)：一种流行的拟牛顿更新公式，能够保持 $H_k$ 的正定性。
![[Pasted image 20250622235841.png]]
DFP 更新 (Davidon-Fletcher-Powell)：另一种更新公式，实际效果不如 BFGS。
收敛性：通常实现超线性收敛。
有限内存 BFGS (L-BFGS)：不需要存储完整的 $H_k^{-1}$ 矩阵，而是存储最近的几次迭代信息来近似，每次迭代的成本降至 $O(nm)$。
优点：计算量小于牛顿法 ($O(n^2)$ 或 $O(nm)$)，收敛速度快于梯度下降法。缺点：需要存储 $H_k$ 或其逆的近似。
![[Pasted image 20250623000030.png]]
内点法（Interior Point Method, IPM）
1.  **线性规划标准型与对偶问题**
 **原问题 (P)**:
 $min \ c^T x$
 $s.t. \ Ax = b, x \ge 0$
 **对偶问题 (D)**:
 $max \ b^T y$
 $s.t. \ A^T y + s = c, s \ge 0$
 **互补条件**: 原始问题和对偶问题的最优解 $(x^*, y^*, s^*)$ 满足 $x_i \cdot s_i = 0, \forall i$。
2.  **对数障碍 (Log-Barrier) 方法**
 **思想**: 使用对数障碍项 $- \mu\sum_{j=1}^n \ln x_j$ 来“替代”原始问题中的不等式约束 $x_j \ge 0$。这个障碍项会惩罚那些接近零的 $x_j$，从而强制迭代过程始终保持在可行域的严格内部 ($x_j > 0$)。
 **障碍目标函数**: 将原始线性规划问题转化为带对数障碍的原始问题：
 $min \ c^T x - \mu \sum_{j=1}^n \ln x_j$
 $s.t. \ Ax = b$
 **几何解释**: 当 $\mu \to \infty$ 时，目标函数趋向于最大化 $\prod x_j$，其最优解是可行域的解析中心；当 $\mu \to 0$ 时，对数障碍项的影响减弱，解趋向于原始问题的最优解。
3.  **最优性条件 (KKT 条件)**
对上述障碍问题的拉格朗日函数 $L(x, y, \mu) = c^T x - y^T(Ax - b) - \mu \sum_{i=1}^n \ln x_i$ 求解驻点条件。
通过定义 $s = \mu X^{-1}e$ (其中 $X = \text{diag}(x_1, \dots, x_n)$, $e$ 是全1向量)，可以得到障碍问题的 KKT 条件：
**原始可行性**: $Ax = b$
**对偶可行性**: $A^T y + s = c$
**扰动互补松弛性**: $XSe = \mu e$ (等价于 $x_i s_i = \mu, \forall i$)
**非负性**: $x, s > 0$
这些条件构成一个非线性方程组，是内点法算法的核心。
**原始-对偶中心路径 (Primal-Dual Central Path)**
中心路径是满足上述 KKT 条件的所有点 $(x(\mu), y(\mu), s(\mu))$ 的集合，其中 $\mu > 0$。
**对偶间隙 (Duality Gap)**: 对于中心路径上的点，对偶间隙 $c^T x - b^T y = x^T s = n\mu$。这意味着随着 $\mu \to 0$，对偶间隙也趋向于零，从而找到原始问题的最优解。
**中心性**: 内点法通过生成一系列既严格可行又“接近”中心路径的迭代点来求解问题，这种“接近”的性质称为中心性。例如，一个 $\beta$-近似解满足 $\left\| \frac{1}{\mu}Xs - e \right\| \le \beta$。
**牛顿法 (Newton's Method)**
由于 KKT 条件是非线性方程组，内点法通常采用牛顿法来求解它。
**牛顿步**: 通过对 KKT 条件进行线性化，得到一个线性方程组，求解该方程组即可得到牛顿步 $(\Delta x, \Delta y, \Delta s)$。
牛顿法具有局部二次收敛性，即当迭代点足够接近最优解时，收敛速度非常快。
内点法（特别是原始-对偶路径追踪算法）的基本框架是一个迭代过程，每次迭代都向着最优解靠近，同时保持解在可行域的内部。
**原始-对偶内点（障碍）算法**
**步骤 0: 初始化**
选择初始数据 $(x_0, y_0, s_0, \mu_0)$，确保 $(x_0, y_0, s_0)$ 是一个严格可行点 ($Ax_0 = b, A^T y_0 + s_0 = c, x_0 > 0, s_0 > 0$) 并且是 $P(\mu_0)$ 的一个 $\beta$-近似解（即靠近中心路径的初始点）。
**步骤 1: 设置当前值**
令当前迭代点 $(\bar{x}, \bar{y}, \bar{s}) = (x_k, y_k, s_k)$，当前障碍参数 $\mu = \mu_k$。
**步骤 2: 缩小 $\mu$**
设置新的目标障碍参数 $\mu' = \alpha \mu$，其中 $\alpha \in (0, 1)$ 是一个缩小因子（例如 $\alpha = 1 - \frac{1}{\sqrt{n}}$ 或其他更激进的值）。这旨在让迭代点向最终的最优解（$\mu \to 0$）靠近。
**步骤 3: 计算牛顿方向**
在当前点 $(\bar{x}, \bar{y}, \bar{s})$ 处，针对以 $\mu'$ 为目标障碍参数的 KKT 方程组，计算原始-对偶牛顿步 $(\Delta x, \Delta y, \Delta s)$。这涉及到求解以下线性方程组：
$A \Delta x = 0$
$A^T \Delta y + \Delta s = 0$
$\bar{S} \Delta x + \bar{X} \Delta s = \bar{X} \bar{S} e - \mu' e$
（其中 $\bar{X} = \text{diag}(\bar{x})$, $\bar{S} = \text{diag}(\bar{s})$）
**步骤 4: 更新所有值**
更新迭代点：$(x', y', s') = (\bar{x}, \bar{y}, \bar{s}) + (\Delta x, \Delta y, \Delta s)$。
**步骤 5: 重置计数器并继续**
设置 $(x_{k+1}, y_{k+1}, s_{k+1}) = (x', y', s')$，$\mu_{k+1} = \mu'$，$k \leftarrow k + 1$。返回步骤 1，重复迭代，直到满足终止条件。
**终止条件**
对偶间隙 $x^T s$ 小于某个预设的容差 $\epsilon$ 时，算法停止。例如，当 $x^T s \le \epsilon$ 时。
严格的终止条件还包括检查原始和对偶残差是否足够小：$\|Ax - b\| \le \epsilon$, $\|A^T y + s - c\| \le \epsilon$。
当 $\mu$ 小于某个阈值 $\bar{\mu}$ 时，可以通过求解一个二次优化问题来识别原始和对偶最优解的结构（严格互补解），从而实现算法的有限终止。
**总体复杂度**: 对于线性规划，内点法的总体复杂度大约为 $O(n^{3.5})$。
**解的性质**: 内点法倾向于找到具有最多非零元素的最优解（高秩解），这与单纯形法倾向于找到基本可行解（通常非零元素数量较少，低秩解）形成对比。
整数规划（决策变量，约束，目标）
建模逻辑关系
“或者-或者”约束：例如，约束 $f_1(x) \le 0$ 或 $f_2(x) \le 0$ 必须成立。
方法：引入一个0-1变量 $y \in {0,1}$。将其转化为 $f_1(x) \le My$ 和 $f_2(x) \le M(1-y)$。当 $y=0$ 时第一个约束生效，当 $y=1$ 时第二个约束生效。
“N个中至少K个成立”约束：例如，给定 $N$ 个约束 $f_i(x) \le 0, i=1,\dots,N$，要求其中至少有 $K$ 个成立。
方法：对每个约束 $f_i(x) \le 0$ 引入一个0-1变量 $y_i \in {0,1}$ ($y_i=1$ 表示该约束成立，$y_i=0$ 表示不成立)。将其转化为 $f_i(x) \le M(1-y_i)$ (对于所有 $i$) 和 $\sum_{i=1}^N y_i = K$。
“如果-那么”约束：例如，如果 $f_1(x) \le 0$ 成立，那么 $f_2(x) \le 0$ 也必须成立。
方法：引入两个0-1变量 $y_1, y_2 \in {0,1}$。将其转化为 $f_1(x) \le M(1-y_1)$, $f_2(x) \le M(1-y_2)$, 并且 $y_2 \ge y_1$。
函数取有限个值：当一个函数 $f(x)$ 只能取预定义的离散值集合 ${d_1, d_2, \dots, d_N}$ 中的一个时。
方法：引入一组0-1变量 $y_i \in {0,1}$，其中 $y_i=1$ 表示 $f(x)=d_i$。将其转化为 $f(x) = \sum_{i=1}^N d_i y_i$ 且 $\sum_{i=1}^N y_i = 1$。
固定成本问题：当生产量 $x_t > 0$ 时会产生固定成本 $f_t$，否则为0。
方法：引入一个0-1变量 $y_t \in {0,1}$ ($y_t=1$ 若 $x_t>0$, $y_t=0$ 若 $x_t=0$)。成本项表示为 $p_t x_t + f_t y_t$，并添加约束 $x_t \le My_t$。目标函数最小化时会自动使 $y_t=0$ 当 $x_t=0$。
线性化非线性项（乘积）
两个0-1变量的乘积 ($z = x \cdot y$, 其中 $x,y \in {0,1}$)：
方法：引入一个辅助的0-1变量 $z \in {0,1}$，并添加线性约束：$z \le x$, $z \le y$, $z \ge x+y-1$。
一个0-1变量与一个连续变量的乘积 ($z = x \cdot y$, 其中 $x \in {0,1}, y \in [0, C]$)：
方法：引入一个辅助的连续变量 $z \in [0,C]$，并添加线性约束：$z \le Cx$, $z \le y$, $z \ge y+C(x-1)$。
处理分段线性函数
方法：将函数定义域划分为若干区间，为每个区间引入一个0-1变量 $w_i \in {0,1}$，表示 $x$ 所在的区间（通过 $\sum w_i=1$ 确保只有一个区间被选中）。通过引入新的连续变量 $x_i = x w_i$ 来线性化乘积项，从而重构分段线性函数的表达式。
线性化最大值/最小值算符
最大值 ($z = \max_{i \in I} {x_i}$)：
方法：添加约束 $z \ge x_i$ 对所有 $i \in I$ 成立。如果目标函数是最小化 $z$ ($\min z$)，则这些约束通常已足够。
最小值 ($z = \min_{i \in I} {x_i}$)：
方法：添加约束 $z \le x_i$ 对所有 $i \in I$ 成立。如果目标函数是最大化 $z$ ($\max z$)，则这些约束通常已足够。
![[Linear Programming 1]]
![[Linear Programming 2]]
![[Linear Programming 3]]
![[Matrix Calculus]]
![[LP对偶]]![[Pasted image 20250623003307.png]]
![[Pasted image 20250623003332.png]]
